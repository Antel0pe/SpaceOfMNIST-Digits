{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ0kSdmm4--I",
        "outputId": "c2917c75-d83e-4148-b6c1-a4c9c7fb049b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# Install all your required packages here using mamba\n",
        "!mamba install -q scikit-learn graph-tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transformation pipeline:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x > 0.5).float()),  # Binarize the image\n",
        "    transforms.Lambda(lambda x: x.view(-1))           # Flatten into a 784-dim vector\n",
        "])\n",
        "\n",
        "# Load the training set (set download=True if running for the first time)\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Get indices for images where the label is 1\n",
        "indices = (mnist_train.targets == 1).nonzero().squeeze()\n",
        "\n",
        "# Create a subset containing only the '1's\n",
        "mnist_train_ones = Subset(mnist_train, indices)\n",
        "\n",
        "print(f\"Total number of '1' images in the training set: {len(mnist_train_ones)}\")\n",
        "\n",
        "# Stack all 784-dim vectors from the filtered dataset\n",
        "all_vectors = torch.stack([img for img, _ in mnist_train_ones])\n",
        "unique_vectors = torch.unique(all_vectors, dim=0)\n",
        "\n",
        "print(f\"Total images in mnist_train_ones: {all_vectors.shape[0]}\")\n",
        "print(f\"Unique images: {unique_vectors.shape[0]}\")\n",
        "\n",
        "if all_vectors.shape[0] == unique_vectors.shape[0]:\n",
        "    print(\"All 784-dimensional vectors are unique.\")\n",
        "else:\n",
        "    print(\"There are duplicates in the 784-dimensional vectors.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1byeYZ5KEx",
        "outputId": "1febd628-d320-4af9-89a6-057be1762096"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of '1' images in the training set: 6742\n",
            "Total images in mnist_train_ones: 6742\n",
            "Unique images: 6726\n",
            "There are duplicates in the 784-dimensional vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import graph_tool as gt\n",
        "\n",
        "def visualize_threshold_graph_gt_weighted(unique_vectors, distance_threshold):\n",
        "    \"\"\"\n",
        "    Builds a weighted graph with graph-tool where each unique vector is a node and an edge is added\n",
        "    between nodes if their Euclidean distance is <= distance_threshold.\n",
        "    The edge is weighted by the real Euclidean distance between the nodes.\n",
        "\n",
        "    Args:\n",
        "        unique_vectors (torch.Tensor or np.array): Array/tensor of shape (n, 784) containing unique images.\n",
        "        distance_threshold (float): Maximum Euclidean distance for adding an edge between two nodes.\n",
        "\n",
        "    Returns:\n",
        "        g (graph_tool.Graph): The constructed weighted graph.\n",
        "    \"\"\"\n",
        "    # Convert tensor to numpy array if necessary.\n",
        "    X = unique_vectors\n",
        "\n",
        "    n = X.shape[0]\n",
        "    print(f\"Building weighted graph for {n} nodes...\")\n",
        "\n",
        "    # Use NearestNeighbors to find all pairs within the distance threshold.\n",
        "    nbrs = NearestNeighbors(radius=distance_threshold, algorithm='auto').fit(X)\n",
        "    distances, indices = nbrs.radius_neighbors(X)\n",
        "\n",
        "    # Create an undirected graph using graph-tool.\n",
        "    g = gt.Graph(directed=False)\n",
        "    g.add_vertex(n)  # Add n vertices\n",
        "\n",
        "    # Build a list of edges with weights\n",
        "    edge_list = []\n",
        "    for i, (dists, neigh) in enumerate(zip(distances, indices)):\n",
        "        # For each neighbor j of node i, add edge only if i < j to avoid duplicates.\n",
        "        edge_list.extend([[i, j, d] for d, j in zip(dists, neigh) if i < j])\n",
        "\n",
        "    # Convert the list of edges to a NumPy array (with float type to hold the weight)\n",
        "    edge_array = np.array(edge_list)\n",
        "\n",
        "    eweight = g.new_ep(\"float\")\n",
        "    # Add all edges at once\n",
        "    g.add_edge_list(edge_array, eprops=[eweight])\n",
        "\n",
        "    print(f\"Weighted graph built with {g.num_edges()} edges.\")\n",
        "    return g\n",
        "\n",
        "# Example usage:\n",
        "g = visualize_threshold_graph_gt_weighted(unique_vectors, distance_threshold=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCK9ZYEI5R-r",
        "outputId": "e47cdfea-00fb-406d-caff-895d4ad661e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building weighted graph for 6726 nodes...\n",
            "Weighted graph built with 4681902 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import graph_tool.all as gt\n",
        "import graph_tool.topology as gt_topo\n",
        "\n",
        "def find_furthest_nodes_double_sweep_largest_component(g, X):\n",
        "    \"\"\"\n",
        "    For a graph g (built with graph-tool) and an array X of shape (n, 784) corresponding to the\n",
        "    unique vectors (with row i corresponding to vertex i), this function:\n",
        "      1. Adds a vertex property to store original indices.\n",
        "      2. Extracts the largest connected component.\n",
        "      3. Finds two nodes that are farthest apart (approximate diameter endpoints) using a double-sweep algorithm\n",
        "         on the subgraph.\n",
        "      4. Computes the shortest path between them in the subgraph.\n",
        "      5. Maps the subgraph vertices back to the original graph using the stored property.\n",
        "      6. Extracts the corresponding 784-d vectors and computes delta vectors along the path.\n",
        "\n",
        "    Returns:\n",
        "      endpoints_sub: Tuple with the two endpoint indices in the subgraph (original indices).\n",
        "      path_indices: List of original graph vertex indices along the shortest path.\n",
        "      path_vectors: Array of the corresponding 784-d vectors for each vertex on the path.\n",
        "      path_deltas: List of delta vectors between consecutive nodes on the path.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Store original vertex indices as a vertex property ---\n",
        "    orig_index = g.new_vertex_property(\"int\")\n",
        "    for v in g.vertices():\n",
        "        orig_index[v] = int(v)\n",
        "\n",
        "    # --- Step 2: Extract the largest connected component ---\n",
        "    comp, hist = gt.label_components(g)\n",
        "    largest_comp = np.argmax(hist)\n",
        "    vfilt = comp.a == largest_comp\n",
        "    g_sub = gt.GraphView(g, vfilt=vfilt)\n",
        "\n",
        "    # Build a mapping from subgraph vertex object to original graph index.\n",
        "    vertex_to_orig = {v: int(orig_index[v]) for v in g_sub.vertices()}\n",
        "    # Also, get a list of subgraph vertices.\n",
        "    vertices_sub = list(g_sub.vertices())\n",
        "    n_sub = len(vertices_sub)\n",
        "    print(f\"Largest connected component has {n_sub} vertices (out of {int(g.num_vertices())} total).\")\n",
        "\n",
        "    # --- Step 3: Double Sweep Algorithm on the subgraph ---\n",
        "    # Use the list of vertices instead of range(n_sub)\n",
        "    v0 = vertices_sub[0]\n",
        "    dmap = gt.shortest_distance(g_sub, source=v0)\n",
        "    # Find the vertex farthest from v0 among vertices_sub.\n",
        "    farthest_idx = np.argmax([dmap[v] for v in vertices_sub])\n",
        "    v1 = vertices_sub[farthest_idx]\n",
        "\n",
        "    dmap2 = gt.shortest_distance(g_sub, source=v1)\n",
        "    farthest_idx2 = np.argmax([dmap2[v] for v in vertices_sub])\n",
        "    v2 = vertices_sub[farthest_idx2]\n",
        "\n",
        "    endpoints_sub = (vertex_to_orig[v1], vertex_to_orig[v2])\n",
        "    diameter_length = dmap2[v2]\n",
        "    print(\"Diameter endpoints in subgraph (original indices):\", endpoints_sub, \"with length:\", diameter_length)\n",
        "\n",
        "    # --- Step 4: Compute the shortest path in the subgraph between v1 and v2 ---\n",
        "    path_vertices_sub, _ = gt_topo.shortest_path(g_sub, source=v1, target=v2)\n",
        "    # path_vertices_sub is a list of vertex objects in g_sub.\n",
        "    print(\"Shortest path in subgraph (vertex objects):\", list(path_vertices_sub))\n",
        "\n",
        "    # --- Step 5: Map subgraph vertex objects back to original graph indices ---\n",
        "    path_indices = [vertex_to_orig[v] for v in path_vertices_sub]\n",
        "    print(\"Mapped original graph vertex indices for path:\", path_indices)\n",
        "\n",
        "    # --- Step 6: Extract the corresponding 784-d vectors and compute delta vectors ---\n",
        "    path_vectors = X[path_indices]\n",
        "    path_deltas = [path_vectors[i+1] - path_vectors[i] for i in range(len(path_vectors) - 1)]\n",
        "\n",
        "    return endpoints_sub, path_indices, path_vectors, path_deltas\n",
        "\n",
        "# Example usage:\n",
        "# Assume unique_vectors is your NumPy array of shape (n, 784)\n",
        "endpoints_sub, path_indices, path_vectors, path_deltas = find_furthest_nodes_double_sweep_largest_component(g, unique_vectors)\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(\"Furthest endpoints in subgraph (original indices):\", endpoints_sub)\n",
        "print(\"Path indices (original graph):\", path_indices)\n",
        "print(\"Path vectors shape:\", np.array(path_vectors).shape)\n",
        "print(\"Number of delta vectors:\", len(path_deltas))\n"
      ],
      "metadata": {
        "id": "foXefCRE5gbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_path_as_images(path_vectors):\n",
        "    \"\"\"\n",
        "    Given an array of 784-d vectors (each representing an MNIST image),\n",
        "    reshapes them into 28x28 images and plots them side-by-side.\n",
        "\n",
        "    Args:\n",
        "        path_vectors (np.array): Array of shape (n_steps, 784)\n",
        "    \"\"\"\n",
        "    num_steps = len(path_vectors)\n",
        "    # Create a figure with one subplot per image along the path.\n",
        "    fig, axs = plt.subplots(1, num_steps, figsize=(num_steps * 2, 2))\n",
        "\n",
        "    # If only one image, ensure axs is iterable.\n",
        "    if num_steps == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    for i, vec in enumerate(path_vectors):\n",
        "        img = vec.reshape(28, 28)\n",
        "        axs[i].imshow(img, cmap='gray')\n",
        "        axs[i].axis(\"off\")\n",
        "        axs[i].set_title(f\"Step {i}\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming path_vectors is the output from the previous function\n",
        "plot_path_as_images(path_vectors)\n"
      ],
      "metadata": {
        "id": "r85EPspZ5-ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume path_deltas is a list of 10 arrays, each of shape (784,)\n",
        "# First, stack them into a 2D array with shape (10, 784)\n",
        "deltas_array = np.stack(path_deltas)  # Shape: (10, 784)\n",
        "\n",
        "# Transpose the array to get shape (784, 10)\n",
        "deltas_by_dimension = deltas_array.T  # Each row now corresponds to a dimension\n",
        "\n",
        "# If you need a list of 784 arrays (each of length 10)\n",
        "deltas_list_by_dimension = [deltas_by_dimension[i, :] for i in range(deltas_by_dimension.shape[0])]\n",
        "\n",
        "print(\"Shape of deltas_array (by path step):\", deltas_array.shape)\n",
        "print(\"Shape of deltas_by_dimension (by dimension):\", deltas_by_dimension.shape)\n"
      ],
      "metadata": {
        "id": "lbPG2ry-6cFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming deltas_by_dimension is a NumPy array of shape (784, 10)\n",
        "indices_with_one = np.where(np.any(deltas_by_dimension != 0, axis=1))[0]\n",
        "print(\"Indices with at least one 1:\", indices_with_one)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming deltas_by_dimension is a numpy array of shape (784, 10)\n",
        "# Compute the count of 1s for each row\n",
        "ones_count = np.sum(deltas_by_dimension != 0, axis=1)\n",
        "\n",
        "# Create a DataFrame with row numbers and count of 1s\n",
        "df = pd.DataFrame({\n",
        "    'Row': np.arange(deltas_by_dimension.shape[0]),\n",
        "    \"Count of 1's\": ones_count\n",
        "})\n",
        "\n",
        "# Sort the DataFrame in descending order by the count of 1s\n",
        "df_sorted = df.sort_values(by=\"Count of 1's\", ascending=False)\n",
        "\n",
        "# Display the sorted table\n",
        "print(df_sorted)\n"
      ],
      "metadata": {
        "id": "OCgxCWCM79Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import graph_tool.all as gt\n",
        "\n",
        "# Label connected components; comp is a property map with component labels,\n",
        "# and hist contains the sizes of each component.\n",
        "comp, hist = gt.label_components(g)\n",
        "\n",
        "# Identify the label corresponding to the largest component.\n",
        "largest_component_label = np.argmax(hist)\n",
        "\n",
        "# Create a vertex filter that is True for vertices in the largest component.\n",
        "vfilt = comp.a == largest_component_label\n",
        "\n",
        "# Create a subgraph view using the vertex filter.\n",
        "g_lcc = gt.GraphView(g, vfilt=vfilt)\n",
        "\n",
        "print(\"Largest connected component has\", g_lcc.num_vertices(), \"vertices and\", g_lcc.num_edges(), \"edges.\")\n",
        "\n",
        "# Get indices of nodes in the largest connected component\n",
        "largest_component_indices = np.where(vfilt)[0]\n",
        "print(\"Indices in largest component:\", largest_component_indices)\n",
        "\n",
        "# Convert indices to a torch tensor (if your unique_vectors is a torch.Tensor)\n",
        "largest_component_indices_tensor = torch.tensor(largest_component_indices, dtype=torch.long)\n",
        "\n",
        "# Filter unique_vectors to get only those in the largest component\n",
        "largest_component_vectors = unique_vectors[largest_component_indices_tensor]\n",
        "print(f\"Using {largest_component_vectors.shape[0]} vectors for training.\")"
      ],
      "metadata": {
        "id": "2VOBIjwm8qU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53257219-974d-4c90-a0bb-a205a12eb63b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest connected component has 6631 vertices and 4681893 edges.\n",
            "Indices in largest component: [   0    1    2 ... 6716 6723 6724]\n",
            "Using 6631 vectors for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a TensorDataset using the unique vectors\n",
        "unique_dataset = TensorDataset(largest_component_vectors)\n",
        "\n",
        "# Define a simple autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: 784 -> 256 -> 128 -> latent_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder: latent_dim -> 128 -> 256 -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 784),\n",
        "            nn.Sigmoid()  # Output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 256\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "latent_dim = 32\n",
        "\n",
        "# Create DataLoader for the unique dataset\n",
        "unique_loader = DataLoader(unique_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up device and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(latent_dim=latent_dim).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Use binary cross entropy since inputs are binary\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop for the autoencoder on unique vectors\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in unique_loader:\n",
        "        images = batch[0].to(device)  # images shape: [batch_size, 784]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(unique_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Extract latent representations for analysis or visualization\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(unique_loader))[0].to(device)\n",
        "    latent_codes = model.get_latent(sample_batch)\n",
        "    print(\"Latent representations shape:\", latent_codes.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLKneguZizB5",
        "outputId": "6275576c-99ca-404a-ef79-009cb082d603"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.4262\n",
            "Epoch [2/50], Loss: 0.1269\n",
            "Epoch [3/50], Loss: 0.1163\n",
            "Epoch [4/50], Loss: 0.1071\n",
            "Epoch [5/50], Loss: 0.0800\n",
            "Epoch [6/50], Loss: 0.0692\n",
            "Epoch [7/50], Loss: 0.0640\n",
            "Epoch [8/50], Loss: 0.0582\n",
            "Epoch [9/50], Loss: 0.0524\n",
            "Epoch [10/50], Loss: 0.0507\n",
            "Epoch [11/50], Loss: 0.0498\n",
            "Epoch [12/50], Loss: 0.0492\n",
            "Epoch [13/50], Loss: 0.0487\n",
            "Epoch [14/50], Loss: 0.0485\n",
            "Epoch [15/50], Loss: 0.0482\n",
            "Epoch [16/50], Loss: 0.0479\n",
            "Epoch [17/50], Loss: 0.0475\n",
            "Epoch [18/50], Loss: 0.0469\n",
            "Epoch [19/50], Loss: 0.0444\n",
            "Epoch [20/50], Loss: 0.0401\n",
            "Epoch [21/50], Loss: 0.0377\n",
            "Epoch [22/50], Loss: 0.0366\n",
            "Epoch [23/50], Loss: 0.0359\n",
            "Epoch [24/50], Loss: 0.0354\n",
            "Epoch [25/50], Loss: 0.0347\n",
            "Epoch [26/50], Loss: 0.0341\n",
            "Epoch [27/50], Loss: 0.0333\n",
            "Epoch [28/50], Loss: 0.0325\n",
            "Epoch [29/50], Loss: 0.0318\n",
            "Epoch [30/50], Loss: 0.0312\n",
            "Epoch [31/50], Loss: 0.0306\n",
            "Epoch [32/50], Loss: 0.0300\n",
            "Epoch [33/50], Loss: 0.0293\n",
            "Epoch [34/50], Loss: 0.0282\n",
            "Epoch [35/50], Loss: 0.0274\n",
            "Epoch [36/50], Loss: 0.0267\n",
            "Epoch [37/50], Loss: 0.0262\n",
            "Epoch [38/50], Loss: 0.0258\n",
            "Epoch [39/50], Loss: 0.0256\n",
            "Epoch [40/50], Loss: 0.0252\n",
            "Epoch [41/50], Loss: 0.0250\n",
            "Epoch [42/50], Loss: 0.0247\n",
            "Epoch [43/50], Loss: 0.0244\n",
            "Epoch [44/50], Loss: 0.0242\n",
            "Epoch [45/50], Loss: 0.0238\n",
            "Epoch [46/50], Loss: 0.0235\n",
            "Epoch [47/50], Loss: 0.0232\n",
            "Epoch [48/50], Loss: 0.0230\n",
            "Epoch [49/50], Loss: 0.0226\n",
            "Epoch [50/50], Loss: 0.0223\n",
            "Latent representations shape: torch.Size([256, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def compare_distances(model, dataset, device, sample_size=50):\n",
        "    # Randomly sample sample_size images from the dataset\n",
        "    indices = torch.randperm(len(dataset))[:sample_size]\n",
        "    sample = torch.stack([dataset[i][0] for i in indices]).to(device)  # shape: [sample_size, 784]\n",
        "\n",
        "    # Get latent representation for the sample\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        latent = model.get_latent(sample)  # shape: [sample_size, latent_dim]\n",
        "\n",
        "    # Compute pairwise Euclidean distances in the original space and latent space\n",
        "    dist_original = torch.cdist(sample, sample, p=2)  # Euclidean (L2) distance\n",
        "    dist_latent = torch.cdist(latent, latent, p=2)      # Euclidean (L2) distance\n",
        "\n",
        "    # Compute cosine similarities (convert to cosine distances if needed)\n",
        "    sample_norm = sample / sample.norm(dim=1, keepdim=True)\n",
        "    latent_norm = latent / latent.norm(dim=1, keepdim=True)\n",
        "    cosine_sim_original = torch.mm(sample_norm, sample_norm.t())\n",
        "    cosine_sim_latent = torch.mm(latent_norm, latent_norm.t())\n",
        "\n",
        "    # For a quick quantitative comparison, compute correlation between the flattened distance matrices\n",
        "    corr_euclidean = np.corrcoef(dist_original.cpu().numpy().flatten(),\n",
        "                                 dist_latent.cpu().numpy().flatten())[0, 1]\n",
        "    print(\"Correlation (Euclidean distances) between original and latent spaces:\", corr_euclidean)\n",
        "\n",
        "    corr_cosine = np.corrcoef(cosine_sim_original.cpu().numpy().flatten(),\n",
        "                              cosine_sim_latent.cpu().numpy().flatten())[0, 1]\n",
        "    print(\"Correlation (Cosine similarities) between original and latent spaces:\", corr_cosine)\n",
        "\n",
        "    return dist_original, dist_latent, cosine_sim_original, cosine_sim_latent\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi3FSlXLjUja",
        "outputId": "aa778bcc-1b3f-49bd-939d-941b562301d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (Euclidean distances) between original and latent spaces: 0.9283457713573456\n",
            "Correlation (Cosine similarities) between original and latent spaces: 0.8830780643562136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (assuming model, unique_dataset, and device are defined as in previous code):\n",
        "dist_original, dist_latent, cosine_sim_original, cosine_sim_latent = compare_distances(model, unique_dataset, device, sample_size=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IffUOsnsgMvw",
        "outputId": "91fffcfc-a65e-4530-f39a-c6a59caf1681"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (Euclidean distances) between original and latent spaces: 0.9172276818756147\n",
            "Correlation (Cosine similarities) between original and latent spaces: 0.9008990810040167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# First, convert the graph-tool edges to a NumPy array of indices.\n",
        "edge_indices = np.array([[int(e.source()), int(e.target())] for e in g.edges()])\n",
        "print(f\"Total number of edges: {edge_indices.shape[0]}\")\n",
        "\n",
        "# Filter for edges where both endpoints are in the largest connected component.\n",
        "# 'vfilt' is a boolean NumPy array of length n (one per unique vector)\n",
        "valid_mask = vfilt[edge_indices[:, 0]] & vfilt[edge_indices[:, 1]]\n",
        "valid_edges = edge_indices[valid_mask]\n",
        "print(f\"Number of edges in largest connected component: {valid_edges.shape[0]}\")\n",
        "\n",
        "# Convert valid_edges to a torch tensor\n",
        "valid_edges_torch = torch.from_numpy(valid_edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrLghGm_eG_V",
        "outputId": "2d2858e5-8026-42c4-a89e-b3a1b6309609"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of edges: 4681902\n",
            "Number of edges in largest connected component: 4681893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say you want to sample 10,000 edges from the valid ones:\n",
        "num_sample_edges = 10000\n",
        "total_valid_edges = valid_edges_torch.shape[0]\n",
        "\n",
        "# Randomly sample indices from valid_edges_torch\n",
        "sample_indices = torch.randperm(total_valid_edges)[:num_sample_edges]\n",
        "sampled_edges = valid_edges_torch[sample_indices]\n",
        "\n",
        "# Vectorized interpolation on the sampled edges\n",
        "src_vectors = unique_vectors[sampled_edges[:, 0]]\n",
        "tgt_vectors = unique_vectors[sampled_edges[:, 1]]\n",
        "interpolated_tensor = (src_vectors + tgt_vectors) / 2.0\n",
        "print(f\"Number of interpolated vectors (sampled): {interpolated_tensor.shape[0]}\")\n",
        "\n",
        "# Create a dataset from the sampled interpolated vectors\n",
        "interpolated_dataset = TensorDataset(interpolated_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMOxYE9BflKe",
        "outputId": "b51904a5-af33-4ea9-b596-c51a663faf0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of interpolated vectors (sampled): 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use your compare_distances function on the interpolated dataset.\n",
        "dist_original, dist_latent, cosine_sim_original, cosine_sim_latent = compare_distances(\n",
        "    model, interpolated_dataset, device, sample_size=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFstYfU5eQN4",
        "outputId": "abd876a9-d95d-43aa-9bd9-89f9db2735df"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (Euclidean distances) between original and latent spaces: 0.9429124656331623\n",
            "Correlation (Cosine similarities) between original and latent spaces: 0.933259506704428\n"
          ]
        }
      ]
    }
  ]
}